"""
Saves cluster maps using multiprocessing
to hdf5 files
"""

import h5py as h5
import functools as ft
from tqdm import tqdm
import os, sys
import unyt
from mpi4py import MPI
#import yt
import numpy as np
#yt.enable_parallelism()
#from yt.funcs import get_pbar
sys.path.append('../')
from map_maker import maps
import cat_reader as cat
from utils import snapshot, data_bin, paths, locate_gals
group_path = "/cosma8/data/dp004/flamingo/Runs/"
run = "HYDRO_FIDUCIAL"
catalogue = "/SOAP"
size = 'L1000N'
res = '3600'

def save_hdf5_map(map_list, cc, halo_ids):
    outputPath="/cosma8/data/dp004/dc-corr2/magnitude_gap/saved_data/L1000N/3600/"
    outputFile=outputPath+'Xray_maps_z0_R500c_20000.hdf5'
    h5file=h5.File(outputFile,'w')

    h5group=h5file.create_group("Cosmology")
    attrName="Critical density [internal units]"
    h5group.attrs[attrName]=cc.rhoCrit
    attrName="H [internal units]"
    h5group.attrs[attrName]=cc.Hz
    attrName="H0 [internal units]"
    h5group.attrs[attrName]=cc.H0
    attrName="Omega_b"
    h5group.attrs[attrName]=cc.OmegaBaryon
    attrName="Omega_m"
    h5group.attrs[attrName]=cc.OmegaMatter
    attrName="Redshift"
    h5group.attrs[attrName]=cc.redshift
    attrName="h"
    h5group.attrs[attrName]=cc.h
    
    h5dset=h5file.create_dataset("VR_ID",np.shape(halo_ids),dtype='uint16',compression="gzip")
    h5dset[:]=halo_ids

    h5dset=h5file.create_dataset("Xray_maps",np.shape(map_list),dtype='f',compression="gzip")

    h5dset[:,:]=map_list
    h5file.close()
    print("------------------------- SAVING COMPLETE -------------------------")

if __name__ == "__main__":
    #breakpoint()
    #method = str(input('use MPI or multithreads (mthreads)? '))
    
    method = 'MPI'
    '''
    z = 0
    catalogue_path, snapshot_path = paths(res, z, group_path, catalogue, run, size, mass='200m',type_='mag', return_ds = False)
    cc = cat.catalogue(catalogue_path, apert='50')
    filename = 'm8_mag_200m_z' + str(z) + '_1e13_50kpc.h5'
    #import pdb; pdb.set_trace()
    path = ("/cosma8/data/dp004/dc-corr2/magnitude_gap/saved_data/" + size + '/' + res + "/" + filename)
    h5file = h5.File(path, 'r')
    h5dset = h5file['data']['host_id']
    halo_ids = h5dset[...]
    h5dset = h5file['data']['M14']
    mag_gap = h5dset[...]
    h5file.close()
    '''
    if method == 'mthreads':
        #save maps in hdf5 file here                                                                                                                                                                                                             
        pool = mp.Pool(processes=15)
        map_list = list(tqdm(pool.map(ft.partial(maps,cc, snapshot_path, z), halo_ids)))
        pool.close()
        pool.join()
        save_hdf5_map(np.array(map_list), cc, np.array(halo_ids))
        quit()
    
    if method == 'MPI':
        
        halo_ids = [1,2,3,4]
        meps = np.meshgrid(np.arange(-256,256,1),np.arange(-256,256,1))
        meps = [meps[0],meps[0],meps[0], meps[0]]

        num_processes = MPI.COMM_WORLD.size
        rank = MPI.COMM_WORLD.rank  # The process ID (integer 0-3 for 4-process run)
        f = h5.File('parallel_test.hdf5', 'w', driver='mpio', comm=MPI.COMM_WORLD)
        dset = f.create_dataset('test', (meps.shape, halo_ids.shape), dtype='f')
        
        for i in range(len(halo_ids)):
            #maps(cc, snapshot_path, 0, _id=halo_id[i])
            if i % num_processes == rank:
                dset[i] = (meps[i],halo_ids[i])
        '''
        
        pbar = yt.get_pbar('Fetching map data', len(halo_ids))
        storage = {}

        for my_storage, i in yt.parallel_objects(range(len(halo_ids[:15000])), storage=storage, dynamic=False):
            pbar.update(i)
            _id = halo_ids[i]
            map_,_,_,_ = maps(int(_id-1), cc.CoP, cc.R500c, snapshot_path, z, 9)
            my_storage.result_id = float(_id)
            my_storage.result = map_

        if yt.is_root():
            save_hdf5_map(list(storage.values()), cc, list(storage.keys()))
                         
        
        '''
